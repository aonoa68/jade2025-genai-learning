# ============================================================
# ãƒªãƒ¡ãƒ‡ã‚£ã‚¢ãƒ«è«–æ–‡2025 - åˆ†æã‚³ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ============================================================
# 
# è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ï¼šå†…ãªã‚‹ä»–è€…ã¨ã®å¯¾è©±ï¼šç”ŸæˆAIã‚’ç”¨ã„ãŸçµ±è¨ˆæ¼”ç¿’ã«ãŠã‘ã‚‹å­¦ç¿’æ”¯æ´ã®å®Ÿè·µå ±å‘Š
# Title: Dialogue with the Inner Other: A Practical Report on Learning Support 
#        Using Generative AI in Statistics Exercises
# 
# æ²è¼‰èªŒï¼šãƒªãƒ¡ãƒ‡ã‚£ã‚¢ãƒ«æ•™è‚²ç ”ç©¶ï¼ˆæ—¥æœ¬ãƒªãƒ¡ãƒ‡ã‚£ã‚¢ãƒ«æ•™è‚²å­¦ä¼šï¼‰
#
# ============================================================
# åˆ†æå†…å®¹ï¼š
#   1. åŸºæœ¬é›†è¨ˆï¼ˆAIæ”¯æ´æŒ‡æ•°ãƒ»ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°ï¼‰- è¡¨2
#   2. ç¸¦æ–­å¤‰åŒ–åˆ†æ - å›³1
#   3. æ„Ÿæƒ…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ - è¡¨3
#   4. èª²é¡Œç‰¹æ€§åˆ†æ - è¡¨5
#   5. æ„Ÿæƒ…Ã—å­¦ç¿’æŒ‡æ•°ã®ç›¸é–¢åˆ†æï¼ˆæ¢ç´¢çš„ãƒ»è«–æ–‡æœªæ²è¼‰ï¼‰
#   6. å¯¾å¿œåˆ†æï¼ˆæ¢ç´¢çš„ãƒ»è«–æ–‡æœªæ²è¼‰ï¼‰
#   7. ä¿¡é ¼æ€§ä¿‚æ•°ã®è¨ˆç®—ï¼ˆå‚è€ƒãƒ»è«–æ–‡æœªæ²è¼‰ï¼‰
#
# æ³¨ï¼š5-7ã¯æ¢ç´¢çš„åˆ†æã¨ã—ã¦å®Ÿæ–½ã—ãŸãŒã€åºŠåŠ¹æœãƒ»è‡ªå·±é¸æŠãƒã‚¤ã‚¢ã‚¹ç­‰ã®
#     ç†ç”±ã«ã‚ˆã‚Šè«–æ–‡æœ¬æ–‡ã«ã¯å«ã‚ã¦ã„ãªã„ã€‚åˆ†æéç¨‹ã®é€æ˜æ€§ã®ãŸã‚å…¬é–‹ã€‚
# ============================================================

# ============================================================
# â‘  æº–å‚™ï¼šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from scipy import stats
from scipy.stats import kruskal, mannwhitneyu, spearmanr
import itertools

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆGoogle Colabç”¨ï¼‰
try:
    import japanize_matplotlib
except ImportError:
    print("japanize_matplotlibãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("pip install japanize_matplotlib ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

# ============================================================
# â‘¡ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# ============================================================
def load_and_clean(path, session):
    """
    å„å›ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å…±é€šå½¢å¼ã«æ•´å½¢
    
    Parameters:
    -----------
    path : str
        Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    session : int
        ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ï¼ˆ5, 7, 9, 11ï¼‰
    
    Returns:
    --------
    df : DataFrame
        æ•´å½¢æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    df = pd.read_excel(path)
    df["session"] = session
    # IDåˆ—ã®ç‰¹å®šï¼ˆåˆ—åã«"ID"ã‚’å«ã‚€åˆ—ï¼‰
    id_col = [c for c in df.columns if "ID" in c][0]
    df = df.rename(columns={id_col: "id"})
    df["id"] = df["id"].astype(str)
    return df


# ============================================================
# â‘¢ Google Colabã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦ä½¿ç”¨ï¼‰
# ============================================================
def upload_files_colab():
    """Google Colabã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    from google.colab import files
    uploaded = files.upload()
    return uploaded


# ============================================================
# â‘£ ãƒ¡ã‚¤ãƒ³åˆ†æã‚¯ãƒ©ã‚¹
# ============================================================
class GenAILearningAnalysis:
    """ç”ŸæˆAIå­¦ç¿’æ”¯æ´ã®åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.df_all = None
        self.dfE_fixed = None
        self.ai_support_items = []
        self.metacog_items = []
        self.emotion_names = ["æœŸå¾…", "é©šã", "å–œã³", "ä¿¡é ¼", "æ€’ã‚Š", "å«Œæ‚ª", "æ‚²ã—ã¿", "æã‚Œ"]
        
    def load_data(self, file_paths):
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨çµåˆ
        
        Parameters:
        -----------
        file_paths : dict
            ã‚»ãƒƒã‚·ãƒ§ãƒ³ç•ªå·ã‚’ã‚­ãƒ¼ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
            ä¾‹: {5: 'session5.xlsx', 7: 'session7.xlsx', ...}
        """
        dfs = []
        for session, path in file_paths.items():
            df = load_and_clean(path, session)
            dfs.append(df)
            print(f"ç¬¬{session}å›: {len(df)}ä»¶")
        
        self.df_all = pd.concat(dfs, ignore_index=True)
        print(f"\nåˆè¨ˆ: {len(self.df_all)}ä»¶")
        
    def define_indices(self):
        """AIæ”¯æ´æŒ‡æ•°ãƒ»ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°ã®æ§‹æˆé …ç›®ã‚’å®šç¾©"""
        # AIæ”¯æ´æŒ‡æ•°ï¼ˆ6é …ç›®ï¼‰
        self.ai_support_items = [
            c for c in self.df_all.columns
            if ("ç”ŸæˆAI" in c) and ("å½¹ç«‹" in c or "ç†è§£" in c or "æ•´ç†" in c)
        ]
        
        # ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°ï¼ˆ9é …ç›®ï¼‰
        self.metacog_items = [
            c for c in self.df_all.columns
            if "èª¬æ˜ã§ãã‚‹" in c or "å¾©ç¿’" in c or "è‰¯ã„è³ªå•" in c or "è‰¯ã„å•ã„" in c
        ]
        
        print(f"AIæ”¯æ´æŒ‡æ•°: {len(self.ai_support_items)}é …ç›®")
        for c in self.ai_support_items:
            print(f"  - {c[:60]}...")
            
        print(f"\nãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°: {len(self.metacog_items)}é …ç›®")
        for c in self.metacog_items:
            print(f"  - {c[:60]}...")
        
        # æŒ‡æ•°ã®ç®—å‡º
        self.df_all["AI_support"] = self.df_all[self.ai_support_items].mean(axis=1)
        self.df_all["Metacognition"] = self.df_all[self.metacog_items].mean(axis=1)
        
    def summary_statistics(self):
        """
        è¡¨2ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥è¦ç´„çµ±è¨ˆé‡
        """
        print("\n" + "="*60)
        print("ã€è¡¨2ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥ AIæ”¯æ´æŒ‡æ•°ãƒ»ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°ã€‘")
        print("="*60)
        
        summary = (
            self.df_all
            .groupby("session")[["AI_support", "Metacognition"]]
            .agg(["mean", "std", "count"])
            .round(3)
        )
        print(summary)
        return summary
    
    def longitudinal_analysis(self, save_fig=True):
        """
        å›³1ï¼šç¸¦æ–­å¤‰åŒ–åˆ†æ
        """
        print("\n" + "="*60)
        print("ã€ç¸¦æ–­åˆ†æã€‘")
        print("="*60)
        
        # è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å›ç­”ã—ãŸIDã‚’ç‰¹å®š
        id_session_df = (
            self.df_all.groupby('id')['session']
            .apply(lambda x: sorted(x.unique().tolist()))
            .reset_index()
        )
        id_session_df['n_sessions'] = id_session_df['session'].apply(len)
        multi_session_ids = id_session_df[id_session_df['n_sessions'] >= 2]
        
        print(f"ç•°ãªã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å›ç­”ã—ãŸID: {len(multi_session_ids)}å")
        print(multi_session_ids.to_string())
        
        valid_ids = multi_session_ids['id'].tolist()
        df_unique = self.df_all.groupby(["id", "session"], as_index=False).mean(numeric_only=True)
        df_long2 = df_unique[df_unique["id"].isin(valid_ids)]
        
        # å›³1ï¼šç¸¦æ–­å¤‰åŒ–ï¼ˆãƒ¢ãƒã‚¯ãƒ­ç‰ˆï¼‰
        styles = [
            {'color': '0.3', 'marker': 'o', 'linestyle': '-', 'fillstyle': 'full'},
            {'color': '0.5', 'marker': 's', 'linestyle': '--', 'fillstyle': 'full'},
            {'color': '0.4', 'marker': '^', 'linestyle': ':', 'fillstyle': 'none'},
        ]
        
        fig, ax = plt.subplots(figsize=(8, 5))
        case_labels = ["Case 1", "Case 2", "Case 3"]
        
        for i, (pid, g) in enumerate(df_long2.groupby("id")):
            ax.plot(g["session"], g["AI_support"],
                    color=styles[i]['color'],
                    linestyle=styles[i]['linestyle'],
                    linewidth=2,
                    marker=styles[i]['marker'],
                    markersize=8,
                    fillstyle=styles[i]['fillstyle'],
                    markeredgewidth=1.5,
                    markeredgecolor=styles[i]['color'],
                    label=case_labels[i],
                    alpha=0.9)
        
        # å¹³å‡ç·šï¼ˆè¿½è·¡å¯èƒ½è€…ï¼‰
        mean_tracking = df_long2.groupby("session")["AI_support"].mean()
        ax.plot(mean_tracking.index, mean_tracking.values,
                color='black', linewidth=2.5, marker='D', markersize=8,
                fillstyle='full', label="å¹³å‡ï¼ˆè¿½è·¡å¯èƒ½è€…ï¼‰", linestyle='-')
        
        # å¹³å‡ç·šï¼ˆå…¨ä½“ï¼‰
        mean_all = self.df_all.groupby("session")["AI_support"].mean()
        ax.plot(mean_all.index, mean_all.values,
                color='0.2', linewidth=2.5, marker='x', markersize=9,
                label="å¹³å‡ï¼ˆå…¨ä½“ï¼‰", linestyle='-.')
        
        ax.set_xlabel("å›", fontsize=12)
        ax.set_ylabel("AIæ”¯æ´æŒ‡æ•°", fontsize=12)
        ax.set_title("ç”ŸæˆAIæ”¯æ´ã®ç¸¦æ–­å¤‰åŒ–ï¼ˆå†…ãªã‚‹ä»–è€…ãƒ¢ãƒ‡ãƒ«ï¼‰", fontsize=14)
        ax.set_xticks([5, 7, 9, 11])
        ax.set_ylim(0, 6)
        ax.legend(loc='lower left', fontsize=10)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_fig:
            plt.savefig("fig1_longitudinal_ai_support_mono.png", dpi=300, bbox_inches='tight')
            print("ğŸ“Š å›³1ã‚’ä¿å­˜ã—ã¾ã—ãŸ: fig1_longitudinal_ai_support_mono.png")
        plt.show()
        
        return df_long2
    
    def emotion_analysis(self):
        """
        è¡¨3ï¼šæ„Ÿæƒ…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        """
        print("\n" + "="*60)
        print("ã€è¡¨3ï¼šæ„Ÿæƒ…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€‘")
        print("="*60)
        
        def normalize_emotion_col(col):
            """æ„Ÿæƒ…åˆ—åã‚’æ­£è¦åŒ–"""
            m = re.search(r"\[(.*?)\]", str(col))
            if not m:
                return col
            name = m.group(1).replace(" ", "").replace("\u3000", "").strip()
            return name
        
        def map_emotion(val):
            """æ„Ÿæƒ…ã®æ–‡å­—åˆ—ã‚’æ•°å€¤ã«ãƒãƒƒãƒ”ãƒ³ã‚°"""
            if pd.isna(val):
                return np.nan
            val = str(val).strip()
            if "æ„Ÿã˜ãªã‹ã£ãŸ" in val:
                return 0
            if "å¼±ã" in val:
                return 1
            if "ã‚„ã‚„" in val:
                return 2
            if "å¼·ã" in val:
                return 3
            return np.nan
        
        def coalesce_duplicate_cols(df, colname):
            """é‡è¤‡åˆ—ã‚’çµ±åˆ"""
            cols = df.loc[:, df.columns == colname]
            if cols.shape[1] == 1:
                return cols.iloc[:, 0]
            return cols.bfill(axis=1).iloc[:, 0]
        
        # æ„Ÿæƒ…åˆ—ã‚’ç‰¹å®š
        emotion_cols_all = [
            c for c in self.df_all.columns 
            if "æ„Ÿæƒ…" in str(c) and "[" in str(c) and "]" in str(c)
        ]
        
        # æ•°å€¤å¤‰æ›
        df_emotion = self.df_all.copy()
        for c in emotion_cols_all:
            df_emotion[c] = df_emotion[c].apply(map_emotion)
        
        # åˆ—åã‚’æ­£è¦åŒ–
        rename_dict = {c: normalize_emotion_col(c) for c in emotion_cols_all}
        dfE = df_emotion.rename(columns=rename_dict)
        
        # æ„Ÿæƒ…åˆ—ã‚’çµ±åˆ
        self.dfE_fixed = pd.DataFrame({"session": dfE["session"]})
        for name in self.emotion_names:
            self.dfE_fixed[name] = coalesce_duplicate_cols(dfE, name)
        
        # å›åˆ¥ã®æ„Ÿæƒ…å¹³å‡
        emotion_summary = self.dfE_fixed.groupby("session")[self.emotion_names].mean().round(3)
        print(emotion_summary)
        
        # Kruskal-Wallisæ¤œå®š
        print("\nâ–  ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“æ¯”è¼ƒï¼ˆKruskal-Wallisæ¤œå®šï¼‰")
        for emotion in self.emotion_names:
            groups = [self.dfE_fixed[self.dfE_fixed["session"]==s][emotion].dropna() for s in [5,7,9,11]]
            if all(len(g) > 0 for g in groups):
                stat, p = kruskal(*groups)
                sig = "**" if p < 0.01 else "*" if p < 0.05 else ""
                print(f"  {emotion}: H={stat:.2f}, p={p:.4f} {sig}")
        
        return emotion_summary
    
    def task_characteristics_analysis(self):
        """
        è¡¨5ï¼šèª²é¡Œç‰¹æ€§åˆ†æ
        """
        print("\n" + "="*60)
        print("ã€è¡¨5ï¼šèª²é¡Œç‰¹æ€§åˆ†æã€‘")
        print("="*60)
        
        TASK_WORDS = {
            7: ["ã‚³ãƒ¼ãƒ‰", "ã‚¨ãƒ©ãƒ¼", "å‹•ã‹ãªã„", "ä¿®æ­£", "æ›¸ãæ–¹"],
            9: ["æŒ‡ç¤º", "æŒ‡å®š", "å…·ä½“çš„", "å½¢å¼", "ç¸¦è»¸", "æ¨ªè»¸", "èª¬æ˜"],
            11: ["ãƒ’ã‚¹ãƒˆ", "åˆ†å¸ƒ", "ãƒ“ãƒ³", "åˆ—", "è³ƒé‡‘", "é¸"],
        }
        
        TASK_LABELS = {7: "ã‚³ãƒ¼ãƒ‰å®Ÿè£…", 9: "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ", 11: "ãƒ‡ãƒ¼ã‚¿é¸æŠ"}
        
        def has_any(text, words):
            t = str(text).lower() if pd.notna(text) else ""
            return any(w.lower() in t for w in words)
        
        def coalesce(row, cols):
            for c in cols:
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    return str(v).strip()
            return ""
        
        # è‡ªç”±è¨˜è¿°ã®çµ±åˆ
        good_cols = [c for c in self.df_all.columns if "è‰¯ã‹ã£ãŸç‚¹" in c or "æ”¹å–„" in c]
        q_cols = [c for c in self.df_all.columns if "ã‚‚ã†ä¸€å•" in c or "è¿½åŠ " in c]
        self.df_all["free_text"] = self.df_all.apply(
            lambda r: coalesce(r, good_cols) + " " + coalesce(r, q_cols), axis=1
        )
        
        # ãƒ•ãƒ©ã‚°ä½œæˆ
        for s, words in TASK_WORDS.items():
            self.df_all[f"flag_s{s}"] = self.df_all.apply(
                lambda r, s=s, words=words: has_any(r["free_text"], words) if r["session"] == s else False,
                axis=1
            )
        
        # çµæœå‡ºåŠ›
        results = []
        for s in [7, 9, 11]:
            flag = f"flag_s{s}"
            subset = self.df_all[self.df_all["session"] == s]
            true_n = subset[flag].sum()
            false_n = len(subset) - true_n
            
            print(f"\nâ–  Session {s}ï¼š{TASK_LABELS[s]}")
            print(f"  ãƒ•ãƒ©ã‚°åˆ†å¸ƒ: True={true_n}, False={false_n}")
            
            if true_n > 0:
                result = subset.groupby(flag)[["AI_support", "Metacognition"]].agg(["mean", "count"]).round(2)
                print(result)
                
                true_ai = subset[subset[flag] == True]["AI_support"].mean()
                false_ai = subset[subset[flag] == False]["AI_support"].mean()
                true_meta = subset[subset[flag] == True]["Metacognition"].mean()
                false_meta = subset[subset[flag] == False]["Metacognition"].mean()
                
                results.append({
                    "session": s,
                    "label": TASK_LABELS[s],
                    "true_n": true_n,
                    "false_n": false_n,
                    "ai_true": true_ai,
                    "ai_false": false_ai,
                    "meta_true": true_meta,
                    "meta_false": false_meta,
                })
        
        return pd.DataFrame(results)
    
    # ============================================================
    # æ¢ç´¢çš„åˆ†æï¼ˆè«–æ–‡æœªæ²è¼‰ï¼‰
    # ============================================================
    
    def correlation_analysis(self):
        """
        æ¢ç´¢çš„åˆ†æï¼šæ„Ÿæƒ…Ã—å­¦ç¿’æŒ‡æ•°ã®ç›¸é–¢
        
        æ³¨ï¼šåºŠåŠ¹æœãƒ»è‡ªå·±é¸æŠãƒã‚¤ã‚¢ã‚¹ã®ãŸã‚è«–æ–‡æœ¬æ–‡ã«ã¯æœªæ²è¼‰
        """
        print("\n" + "="*60)
        print("ã€æ¢ç´¢çš„åˆ†æï¼šæ„Ÿæƒ…Ã—å­¦ç¿’æŒ‡æ•°ã®ç›¸é–¢ã€‘")
        print("æ³¨ï¼šåºŠåŠ¹æœãƒ»è‡ªå·±é¸æŠãƒã‚¤ã‚¢ã‚¹ã®ãŸã‚è«–æ–‡æœ¬æ–‡ã«ã¯æœªæ²è¼‰")
        print("="*60)
        
        df_corr = self.dfE_fixed.copy()
        df_corr["AI_support"] = self.df_all["AI_support"].values
        df_corr["Metacognition"] = self.df_all["Metacognition"].values
        
        print("\nâ–  ç›¸é–¢ä¿‚æ•°ã¨æœ‰æ„æ€§æ¤œå®šï¼ˆSpearman's Ïï¼‰")
        results = []
        for emotion in self.emotion_names:
            for outcome in ["AI_support", "Metacognition"]:
                valid = df_corr[[emotion, outcome]].dropna()
                if len(valid) > 10:
                    rho, p = spearmanr(valid[emotion], valid[outcome])
                    sig = "**" if p < 0.01 else "*" if p < 0.05 else ""
                    results.append({
                        "æ„Ÿæƒ…": emotion, "æŒ‡æ¨™": outcome,
                        "Ï": round(rho, 3), "p": round(p, 4), "æœ‰æ„": sig, "N": len(valid)
                    })
        
        corr_df = pd.DataFrame(results)
        print(corr_df.to_string(index=False))
        
        # åº¦æ•°åˆ†å¸ƒï¼ˆåºŠåŠ¹æœã®ç¢ºèªï¼‰
        print("\nâ–  æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã®åº¦æ•°åˆ†å¸ƒï¼ˆåºŠåŠ¹æœã®ç¢ºèªï¼‰")
        for emotion in self.emotion_names:
            total = df_corr[emotion].notna().sum()
            n_zero = (df_corr[emotion] == 0).sum()
            pct_zero = n_zero / total * 100 if total > 0 else 0
            effect = "âš ï¸åºŠåŠ¹æœ" if pct_zero >= 50 else ""
            print(f"  {emotion}: 0ã®å‰²åˆ={pct_zero:.1f}% {effect}")
        
        return corr_df
    
    def correspondence_analysis(self):
        """
        æ¢ç´¢çš„åˆ†æï¼šå¯¾å¿œåˆ†æ
        
        æ³¨ï¼šãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¸æ˜ç­ãªãŸã‚è«–æ–‡æœ¬æ–‡ã«ã¯æœªæ²è¼‰
        """
        try:
            import prince
        except ImportError:
            print("princeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print("pip install prince ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return None
        
        print("\n" + "="*60)
        print("ã€æ¢ç´¢çš„åˆ†æï¼šå¯¾å¿œåˆ†æã€‘")
        print("æ³¨ï¼šãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¸æ˜ç­ãªãŸã‚è«–æ–‡æœ¬æ–‡ã«ã¯æœªæ²è¼‰")
        print("="*60)
        
        learning_items = self.ai_support_items + self.metacog_items
        
        df_ca = self.df_all.copy()
        df_ca["respondent_id"] = df_ca["session"].astype(str) + "_" + df_ca.index.astype(str)
        
        df_learning = df_ca[["respondent_id", "session"] + learning_items].copy()
        df_learning["valid_count"] = df_learning[learning_items].notna().sum(axis=1)
        df_learning_valid = df_learning[df_learning["valid_count"] >= 2].copy()
        
        print(f"æœ‰åŠ¹å›ç­”æ•°: {len(df_learning_valid)}")
        
        df_matrix = df_learning_valid.set_index("respondent_id")[learning_items].copy()
        for col in df_matrix.columns:
            col_mean = df_matrix[col].mean()
            df_matrix[col] = df_matrix[col].fillna(col_mean)
        
        ca = prince.CA(n_components=2, random_state=42)
        ca = ca.fit(df_matrix)
        
        print(f"ç¬¬1è»¸ã®å¯„ä¸ç‡: {ca.percentage_of_variance_[0]:.1f}%")
        print(f"ç¬¬2è»¸ã®å¯„ä¸ç‡: {ca.percentage_of_variance_[1]:.1f}%")
        print(f"ç´¯ç©å¯„ä¸ç‡: {sum(ca.percentage_of_variance_[:2]):.1f}%")
        
        return ca
    
    def reliability_analysis(self):
        """
        å‚è€ƒï¼šä¿¡é ¼æ€§ä¿‚æ•°ã®è¨ˆç®—
        
        æ³¨ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã«è³ªå•ãŒç•°ãªã‚‹ãŸã‚ã€å…¨ä½“ã®Î±ä¿‚æ•°ã¯ç®—å‡ºä¸å¯
        """
        print("\n" + "="*60)
        print("ã€å‚è€ƒï¼šä¿¡é ¼æ€§ä¿‚æ•°ã€‘")
        print("æ³¨ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã«è³ªå•ãŒç•°ãªã‚‹ãŸã‚ã€å…¨ä½“ã®Î±ä¿‚æ•°ã¯ç®—å‡ºä¸å¯")
        print("="*60)
        
        def cronbachs_alpha(df_items):
            df_clean = df_items.dropna()
            k = df_clean.shape[1]
            if k < 2 or len(df_clean) < 2:
                return np.nan
            item_variances = df_clean.var(axis=0, ddof=1)
            total_variance = df_clean.sum(axis=1).var(ddof=1)
            if total_variance == 0:
                return np.nan
            return (k / (k - 1)) * (1 - item_variances.sum() / total_variance)
        
        df_ai_items = self.df_all[self.ai_support_items].copy()
        df_meta_items = self.df_all[self.metacog_items].copy()
        
        alpha_ai = cronbachs_alpha(df_ai_items)
        alpha_meta = cronbachs_alpha(df_meta_items)
        
        print(f"AIæ”¯æ´æŒ‡æ•°: æœ‰åŠ¹N={df_ai_items.dropna().shape[0]}, Î±={alpha_ai:.3f}")
        print(f"ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°: æœ‰åŠ¹N={df_meta_items.dropna().shape[0]}, Î±={alpha_meta:.3f}")
        
        return {"AI_support": alpha_ai, "Metacognition": alpha_meta}
    
    def run_all_analyses(self, save_figures=True):
        """å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œ"""
        print("\n" + "="*60)
        print("ğŸ”¬ åˆ†æé–‹å§‹")
        print("="*60)
        
        # æŒ‡æ¨™å®šç¾©
        self.define_indices()
        
        # è«–æ–‡æ²è¼‰åˆ†æ
        self.summary_statistics()
        self.longitudinal_analysis(save_fig=save_figures)
        self.emotion_analysis()
        self.task_characteristics_analysis()
        
        # æ¢ç´¢çš„åˆ†æï¼ˆè«–æ–‡æœªæ²è¼‰ï¼‰
        self.correlation_analysis()
        self.correspondence_analysis()
        self.reliability_analysis()
        
        print("\n" + "="*60)
        print("ğŸ”š åˆ†æå®Œäº†")
        print("="*60)
        print(f"""
âœ… ãƒ‡ãƒ¼ã‚¿: {len(self.df_all)}ä»¶ï¼ˆ4ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰
âœ… AIæ”¯æ´æŒ‡æ•°: {len(self.ai_support_items)}é …ç›®
âœ… ãƒ¡ã‚¿èªçŸ¥æŒ‡æ•°: {len(self.metacog_items)}é …ç›®
""")


# ============================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ============================================================
if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹
    print("""
    ============================================================
    ä½¿ç”¨æ–¹æ³•
    ============================================================
    
    # 1. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    analysis = GenAILearningAnalysis()
    
    # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    file_paths = {
        5: 'ã€å›ç­”ã€‘ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸå­¦ç¿’æ”¯æ´ã«é–¢ã™ã‚‹ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆç¬¬5å›ï¼‰.xlsx',
        7: 'ã€å›ç­”ã€‘ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸå­¦ç¿’æ”¯æ´ã«é–¢ã™ã‚‹ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆç¬¬7å›ï¼šäººå£çµ±è¨ˆæ¼”ç¿’ï¼‰.xlsx',
        9: 'ã€å›ç­”ã€‘ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸå­¦ç¿’æ”¯æ´ã«é–¢ã™ã‚‹ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆç¬¬9å›ï¼šåŠ´åƒçµ±è¨ˆæ¼”ç¿’ï¼‰.xlsx',
        11: 'ã€å›ç­”ã€‘ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸå­¦ç¿’æ”¯æ´ã«é–¢ã™ã‚‹ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆç¬¬11å›ï¼šè³ƒé‡‘çµ±è¨ˆæ¼”ç¿’ï¼‰.xlsx',
    }
    analysis.load_data(file_paths)
    
    # 3. å…¨åˆ†æå®Ÿè¡Œ
    analysis.run_all_analyses()
    
    # ã¾ãŸã¯å€‹åˆ¥ã«å®Ÿè¡Œ
    analysis.define_indices()
    analysis.summary_statistics()      # è¡¨2
    analysis.longitudinal_analysis()   # å›³1
    analysis.emotion_analysis()        # è¡¨3
    analysis.task_characteristics_analysis()  # è¡¨5
    """)
